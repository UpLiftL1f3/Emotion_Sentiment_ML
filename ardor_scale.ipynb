{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "335a660d-2d4e-4f8f-98a8-e70c7bbc3699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping classical models: [Errno 2] No such file or directory: './models/logreg_model.pkl'\n",
      "‚úÖ DistilBERT loaded successfully!\n",
      "‚ö†Ô∏è Skipping RoBERTa: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './models/roberta_ardor'. Use `repo_type` argument if needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabasiddiqi/Library/Python/3.9/lib/python/site-packages/gradio/components/dropdown.py:188: UserWarning: The value passed into gr.Dropdown() is not in the list of choices. Please update the list of choices to include: Logistic Regression or set allow_custom_value=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on public URL: https://70deb95c7d54515f5d.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://70deb95c7d54515f5d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import gradio as gr\n",
    "# import numpy as np\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# # DistilBERT model defined\n",
    "# class MultiHeadDistilBert(nn.Module):\n",
    "#     def __init__(self, base_name, num_labels_sent, num_labels_emot):\n",
    "#         super().__init__()\n",
    "#         self.encoder = AutoModel.from_pretrained(base_name)\n",
    "#         hidden = 768\n",
    "#         self.dropout = nn.Dropout(0.2)\n",
    "#         self.classifier_sent = nn.Linear(hidden, num_labels_sent)\n",
    "#         self.classifier_emot = nn.Linear(hidden, num_labels_emot)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         cls = out.last_hidden_state[:, 0]\n",
    "#         cls = self.dropout(cls)\n",
    "#         return {\n",
    "#             \"logits_sent\": self.classifier_sent(cls),\n",
    "#             \"logits_emot\": self.classifier_emot(cls)\n",
    "#         }\n",
    "\n",
    "# # --- Load the saved model and tokenizer ---\n",
    "# distil_dir = \"./distilbert_ardor_saved\"   # üëà adjust this path\n",
    "# tokenizer = AutoTokenizer.from_pretrained(distil_dir)\n",
    "# model = MultiHeadDistilBert(\"distilbert-base-uncased\", 2, 6)\n",
    "# model.load_state_dict(torch.load(f\"{distil_dir}/pytorch_model.bin\", map_location=\"cpu\"))\n",
    "# model.eval()\n",
    "# print(\"‚úÖ DistilBERT model loaded successfully!\")\n",
    "\n",
    "# # --- Label maps ---\n",
    "# id2label_sent = {0: \"Negative\", 1: \"Positive\"}\n",
    "# id2label_emot = {0: \"joy\", 1: \"sad\", 2: \"anger\", 3: \"fear\", 4: \"love\", 5: \"surprise\"}\n",
    "\n",
    "# def softmax_temp(x, T=1.5):\n",
    "#     return torch.nn.functional.softmax(x / T, dim=-1)\n",
    "\n",
    "# # --- Prediction ---\n",
    "# def predict_distilbert(text):\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "#     with torch.no_grad():\n",
    "#         out = model(**inputs)\n",
    "#         ps = softmax_temp(out[\"logits_sent\"]).cpu().numpy()[0]\n",
    "#         pe = softmax_temp(out[\"logits_emot\"]).cpu().numpy()[0]\n",
    "#     sent_idx = int(np.argmax(ps))\n",
    "#     sent_label = id2label_sent[sent_idx]\n",
    "#     sent_conf = float(ps[sent_idx])\n",
    "#     top_idx = np.argsort(-pe)[:3]\n",
    "#     emotions = [(id2label_emot[i], float(pe[i])) for i in top_idx]\n",
    "#     return sent_label, sent_conf, emotions\n",
    "\n",
    "# # --- Gradio App ---\n",
    "# def analyze_text(text):\n",
    "#     label, conf, emo = predict_distilbert(text)\n",
    "#     ardor = round(conf * 100, 2)\n",
    "#     emo_str = \" | \".join([f\"{e[0]}: {e[1]:.2f}\" for e in emo])\n",
    "#     return f\"**Sentiment:** {label} ({ardor}%)\", f\"**Ardor Scale:** {ardor}%\", f\"**Top Emotions:** {emo_str}\"\n",
    "\n",
    "# iface = gr.Interface(\n",
    "#     fn=analyze_text,\n",
    "#     inputs=gr.Textbox(label=\"Enter text to analyze\"),\n",
    "#     outputs=[\n",
    "#         gr.Markdown(label=\"Sentiment\"),\n",
    "#         gr.Markdown(label=\"Ardor Scale\"),\n",
    "#         gr.Markdown(label=\"Top Emotions\")\n",
    "#     ],\n",
    "#     title=\"üé≠ The Ardor Scale ‚Äì DistilBERT Demo\",\n",
    "#     examples=[\n",
    "#         [\"I absolutely love this project!\"],\n",
    "#         [\"This is terrible and makes me so upset.\"],\n",
    "#         [\"It‚Äôs okay, nothing special.\"],\n",
    "#         [\"I can‚Äôt wait for the concert tonight!\"]\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# iface.launch(share=True)\n",
    "# =================================================================================\n",
    "\n",
    "\n",
    "# Unified website for all models\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import joblib\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "\n",
    "# 1. Define MultiHeadDistilBert\n",
    "\n",
    "class MultiHeadDistilBert(nn.Module):\n",
    "    def __init__(self, base_name, num_labels_sent, num_labels_emot):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(base_name)\n",
    "        hidden = 768\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier_sent = nn.Linear(hidden, num_labels_sent)\n",
    "        self.classifier_emot = nn.Linear(hidden, num_labels_emot)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls = out.last_hidden_state[:, 0]\n",
    "        cls = self.dropout(cls)\n",
    "        return {\n",
    "            \"logits_sent\": self.classifier_sent(cls),\n",
    "            \"logits_emot\": self.classifier_emot(cls)\n",
    "        }\n",
    "\n",
    "# 2. Load models\n",
    "\n",
    "models_dir = \"./models\"\n",
    "\n",
    "# --- Logistic Regression + SVM (classical baselines) ---\n",
    "try:\n",
    "    logreg_model = joblib.load(os.path.join(models_dir, \"logreg_model.pkl\"))\n",
    "    svm_model = joblib.load(os.path.join(models_dir, \"svm_model.pkl\"))\n",
    "    vectorizer = joblib.load(os.path.join(models_dir, \"tfidf_vectorizer.pkl\"))\n",
    "    has_classical = True\n",
    "    print(\"‚úÖ Classical models loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Skipping classical models:\", e)\n",
    "    has_classical = False\n",
    "    logreg_model = svm_model = vectorizer = None\n",
    "\n",
    "# --- DistilBERT (MultiHead) ---\n",
    "try:\n",
    "    distil_dir = \"./distilbert_ardor_saved\"\n",
    "    distil_tokenizer = AutoTokenizer.from_pretrained(distil_dir)\n",
    "    distil_model = MultiHeadDistilBert(\"distilbert-base-uncased\", 2, 6)\n",
    "    distil_model.load_state_dict(torch.load(f\"{distil_dir}/pytorch_model.bin\", map_location=\"cpu\"))\n",
    "    distil_model.eval()\n",
    "    has_distil = True\n",
    "    print(\"‚úÖ DistilBERT loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Skipping DistilBERT:\", e)\n",
    "    has_distil = False\n",
    "\n",
    "# --- RoBERTa ---\n",
    "try:\n",
    "    roberta_dir = os.path.join(models_dir, \"roberta_ardor\")\n",
    "    roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_dir)\n",
    "    roberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_dir)\n",
    "    roberta_model.eval()\n",
    "    has_roberta = True\n",
    "    print(\"‚úÖ RoBERTa loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Skipping RoBERTa:\", e)\n",
    "    has_roberta = False\n",
    "\n",
    "# 3. Label maps\n",
    "\n",
    "id2label_sent = {0: \"Negative\", 1: \"Positive\"}\n",
    "id2label_emot = {0: \"joy\", 1: \"sad\", 2: \"anger\", 3: \"fear\", 4: \"love\", 5: \"surprise\"}\n",
    "\n",
    "def softmax_temp(x, T=1.5):\n",
    "    return torch.nn.functional.softmax(x / T, dim=-1)\n",
    "\n",
    "# 4. Prediction functions\n",
    "\n",
    "def predict_classical(model, text):\n",
    "    X = vectorizer.transform([text])\n",
    "    probs = model.predict_proba(X)[0]\n",
    "    idx = np.argmax(probs)\n",
    "    label = \"Positive\" if idx == 1 else \"Negative\"\n",
    "    conf = float(probs[idx])\n",
    "    return label, conf, []\n",
    "\n",
    "def predict_distilbert(text):\n",
    "    inputs = distil_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        out = distil_model(**inputs)\n",
    "        ps = softmax_temp(out[\"logits_sent\"]).cpu().numpy()[0]\n",
    "        pe = softmax_temp(out[\"logits_emot\"]).cpu().numpy()[0]\n",
    "    sent_idx = int(np.argmax(ps))\n",
    "    sent_label = id2label_sent[sent_idx]\n",
    "    sent_conf = float(ps[sent_idx])\n",
    "    top_idx = np.argsort(-pe)[:3]\n",
    "    emotions = [(id2label_emot[i], float(pe[i])) for i in top_idx]\n",
    "    return sent_label, sent_conf, emotions\n",
    "\n",
    "def predict_roberta(text):\n",
    "    inputs = roberta_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        logits = roberta_model(**inputs).logits\n",
    "        ps = softmax_temp(logits).cpu().numpy()[0]\n",
    "    idx = int(np.argmax(ps))\n",
    "    label = id2label_sent[idx]\n",
    "    conf = float(ps[idx])\n",
    "    return label, conf, []\n",
    "\n",
    "# 5. Unified prediction router\n",
    "\n",
    "def analyze_text(text, model_choice):\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return \"‚ö†Ô∏è Please enter text.\", \"\", \"\"\n",
    "\n",
    "    if model_choice == \"Logistic Regression\" and has_classical:\n",
    "        label, conf, emo = predict_classical(logreg_model, text)\n",
    "    elif model_choice == \"SVM\" and has_classical:\n",
    "        label, conf, emo = predict_classical(svm_model, text)\n",
    "    elif model_choice == \"DistilBERT\" and has_distil:\n",
    "        label, conf, emo = predict_distilbert(text)\n",
    "    elif model_choice == \"RoBERTa\" and has_roberta:\n",
    "        label, conf, emo = predict_roberta(text)\n",
    "    else:\n",
    "        return \"‚ö†Ô∏è Model not available or not loaded.\", \"\", \"\"\n",
    "\n",
    "    ardor = round(conf * 100, 2)\n",
    "    emo_str = \" | \".join([f\"{e[0]}: {e[1]:.2f}\" for e in emo]) if emo else \"N/A\"\n",
    "    return f\"**Sentiment:** {label} ({ardor}%)\", f\"**Ardor Scale:** {ardor}%\", f\"**Top Emotions:** {emo_str}\"\n",
    "\n",
    "# ======================================================\n",
    "# 6. Gradio Interface for Front end\n",
    "\n",
    "available_models = []\n",
    "if has_classical:\n",
    "    available_models.extend([\"Logistic Regression\", \"SVM\"])\n",
    "if has_distil:\n",
    "    available_models.append(\"DistilBERT\")\n",
    "if has_roberta:\n",
    "    available_models.append(\"RoBERTa\")\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=analyze_text,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Enter text to analyze\"),\n",
    "        gr.Dropdown(available_models, label=\"Select Model\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Markdown(label=\"Sentiment\"),\n",
    "        gr.Markdown(label=\"Ardor Scale\"),\n",
    "        gr.Markdown(label=\"Top Emotions\")\n",
    "    ],\n",
    "    title=\"üé≠ The Ardor Scale ‚Äì Multi-Model Sentiment & Emotion Analyzer\",\n",
    "    description=\"Compare sentiment, Ardor confidence, and emotion predictions across multiple models.\",\n",
    "    examples=[\n",
    "        [\"I absolutely love this project!\", \"DistilBERT\"],\n",
    "        [\"This is terrible and makes me so upset.\", \"DistilBERT\"],\n",
    "        [\"It‚Äôs okay, nothing special.\", \"Logistic Regression\"],\n",
    "        [\"I'm so excited for the concert tonight!\", \"DistilBERT\"]\n",
    "    ],\n",
    "    allow_flagging=\"never\"\n",
    ")\n",
    "\n",
    "# 7. Launch App\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b34d817-6c27-4889-9c33-3fb954579519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
